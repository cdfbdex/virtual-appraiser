{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ElAlgoritmoEsCorrecto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDqvqZOsW__1"
      },
      "source": [
        "# **Virtual-Appraiser**\n",
        "\n",
        "\n",
        "La idea del algoritmo Virtual-Appraiser es la de emular la capacidad de búsqueda y análisis de información de los avaluadores inmobiliarios utilizando data histórica de ventas, datos abiertos y técnicas de web scrapping  basado en análisis de imágenes mediante técnicas de reconocimiento óptico de caracteres y de aprendizaje profundo (DeepLearning).\n",
        "\n",
        "**Algoritmo**: *Virtual-Appraiser*\n",
        "\n",
        "**Entrada**: dirección del inmueble (DI), Valor Compra M2 Inmueble (VCI), área (A), número de cuartos (NC), número de baños (NB), número de parqueadores (NG), número de pisos (NP), estrato (E), tipo de inmueble (TI), polígono de estaciones Transmilenio (PET), polígono de centros comerciales (PCC), polígono de parques (PP), polígono de CAIs (PC), dataset ventas de inmuebles (DVI), Página Web de Oferta de Inmuebles (PWOI).\n",
        "\n",
        "**Salida**: Estimación de Precio M2 para Venta, PM2, ($/m2).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_VBX1AfLMqf"
      },
      "source": [
        "# Configuring environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZKuHyZjDAek"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "!pip install keras==2.1.0\n",
        "!pip show keras\n",
        "!pip install keras==2.1.0\n",
        "!pip show keras\n",
        "\n",
        "!pip3 install geocoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7lx_Lbdfzvt"
      },
      "source": [
        "!sudo apt install tesseract-ocr libtesseract-dev libmagickwand-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0cDtVgaf2Xl"
      },
      "source": [
        "!pip install pytesseract"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4e7Q87SLKRa"
      },
      "source": [
        "# Importing Libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysK7Im8GEMWD"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import geocoder\n",
        "from zipfile import ZipFile\n",
        "import cv2\n",
        "from scipy import spatial\n",
        "\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import skimage\n",
        "import glob\n",
        "\n",
        "from random import sample\n",
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "%matplotlib inline \n",
        "\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "import pytesseract\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njwYv0PFK3jS"
      },
      "source": [
        "# Mounting G-Drive and Setting Data and Models path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy-EzNB3K5lO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhupJ1wkK9bF"
      },
      "source": [
        "#Change this according to your path\n",
        "dataPath = '/content/drive/MyDrive/INTERPRETABLE_MACHINE_LEARNING_PROJECTS/'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FE7I8f1J3YB"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPZFoNLWJ40z"
      },
      "source": [
        "# Need validations and too much work\n",
        "def geodecoder(address):\n",
        "  loc = geocoder.osm(address+', Bogotá, Colombia')\n",
        "  lat, lon = loc.latlng\n",
        "  return lat,lon"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gft0KpexUjEr"
      },
      "source": [
        "# Confidence intervals estimation\n",
        "def bootstrap(dataset, confidence=0.95, iterations=10000,\n",
        "              sample_size=1.0, statistic=np.mean):\n",
        "    \"\"\"\n",
        "    Bootstrap the confidence intervals for a given sample of a population\n",
        "    and a statistic.\n",
        "    Args:\n",
        "        dataset: A list of values, each a sample from an unknown population\n",
        "        confidence: The confidence value (a float between 0 and 1.0)\n",
        "        iterations: The number of iterations of resampling to perform\n",
        "        sample_size: The sample size for each of the resampled (0 to 1.0\n",
        "                     for 0 to 100% of the original data size)\n",
        "        statistic: The statistic to use. This must be a function that accepts\n",
        "                   a list of values and returns a single value.\n",
        "    Returns:\n",
        "        Returns the upper and lower values of the confidence interval.\n",
        "    \"\"\"\n",
        "    stats = list()\n",
        "    n_size = int(len(dataset) * sample_size)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        # Sample (with replacement) from the given dataset\n",
        "        sample = resample(dataset, n_samples=n_size)\n",
        "        # Calculate user-defined statistic and store it\n",
        "        stat = statistic(sample)\n",
        "        stats.append(stat)\n",
        "\n",
        "    # Sort the array of per-sample statistics and cut off ends\n",
        "    ostats = sorted(stats)\n",
        "    lval = np.percentile(ostats, ((1 - confidence) / 2) * 100)\n",
        "    uval = np.percentile(ostats, (confidence + ((1 - confidence) / 2)) * 100)\n",
        "\n",
        "    return (lval, uval)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJzlhypOJ23v"
      },
      "source": [
        "#Preprocessing Database: Naive proposals\n",
        "def preprocessData(dfDVI):\n",
        "  dfTrain = dfDVI.copy()\n",
        "  #Drop unnecessary columns\n",
        "  dfTrain = dfTrain[[val for val in dfTrain.columns.tolist() if not (val in ['id'])]]\n",
        "\n",
        "  # This field should be fixed by questioning to data owners/generators\n",
        "  print('Before')\n",
        "  colfix = 'tiponegocio'\n",
        "  print(dfTrain[colfix].unique())\n",
        "  dfTrain[colfix] = dfTrain[colfix].fillna('Venta')\n",
        "  dfTrain[colfix] = [val.lower() for val in dfTrain[colfix]]\n",
        "  print('After')\n",
        "  print(dfTrain[colfix].unique())\n",
        "\n",
        "  # This field should be fixed by questioning to data owners/generators\n",
        "  print('Before')\n",
        "  colfix = 'piso'\n",
        "  print(dfTrain[colfix].unique())\n",
        "  dfTrain[colfix] = dfTrain[colfix].fillna(1.0)\n",
        "  print('After')\n",
        "  print(dfTrain[colfix].unique())\n",
        "\n",
        "  # This field should be fixed by questioning to data owners/generators\n",
        "  print('Before')\n",
        "  colfix = 'banos'\n",
        "  print(dfTrain[colfix].unique())\n",
        "  dfTrain[colfix] = dfTrain[colfix].fillna(1.0)\n",
        "  print('After')\n",
        "  print(dfTrain[colfix].unique())\n",
        "\n",
        "  # This field should be fixed by questioning to data owners/generators\n",
        "  print('Before')\n",
        "  colfix = 'habitaciones'\n",
        "  print(dfTrain[colfix].unique())\n",
        "  dfTrain[colfix] = dfTrain[colfix].fillna(1.0)\n",
        "  print('After')\n",
        "  print(dfTrain[colfix].unique())\n",
        "\n",
        "  # We assume N.A. as no information\n",
        "  print('Before')\n",
        "  colfix = 'vista'\n",
        "  print(dfTrain[colfix].unique())\n",
        "  dfTrain[colfix] = dfTrain[colfix].fillna('N.A.')\n",
        "  print('After')\n",
        "  print(dfTrain[colfix].unique())\n",
        "\n",
        "  # This field should be precised with house owners\n",
        "  # This is a naive preprocessing\n",
        "  print('Before')\n",
        "  colfix = 'tiempodeconstruido'\n",
        "  print(dfTrain[colfix].unique())\n",
        "  dictFixBuiltTime=dict(zip([float('nan'), 'Entre 10 y 20 años', 'Entre 0 y 5 años',\n",
        "                            'Entre 5 y 10 años', 'Más de 20 años', '1 a 8 años',\n",
        "                            '16 a 30 años', '9 a 15 años', 'Más de 30 años',\n",
        "                            'Menos de 1 año', 'Remodelado', 'ntre 0 y 5 años'],\n",
        "                            ['0-5', '10-15', '0-5', '5-10', '20-25', '0-5', '15-20',\n",
        "                            '10-15', '30-35', '0-5', 'Remodelado', '0-5']))\n",
        "  dfTrain[colfix] = dfTrain[colfix].replace(dictFixBuiltTime)\n",
        "  print('After')\n",
        "  print(dfTrain[colfix].unique())\n",
        "\n",
        "  # This field could be automatically filtered using polygons of neighboors in Bogotá\n",
        "  # and using Point in Polygon\n",
        "  print('Before')\n",
        "  colfix = 'estrato'\n",
        "  print(dfTrain[colfix].unique())\n",
        "  print(dfTrain[colfix].value_counts())\n",
        "  dfTrain[colfix] = dfTrain[colfix].fillna(0.0)\n",
        "  print('After')\n",
        "  print(dfTrain[colfix].unique())\n",
        "\n",
        "  # This field needs to be created according to challenge\n",
        "  dfTrain['PriceM2'] = [price/area if area > 0.0 else float('nan') for price, area in zip(dfTrain['valorventa'],dfTrain['area'])]\n",
        "\n",
        "  # Drop observations where area is nan and fill all nan values with zeros. This is a very naive filtering :)\n",
        "  dfTrain.dropna(subset=['area'],inplace=True)\n",
        "  dfTrain.reset_index(inplace=True)\n",
        "  dfTrain = dfTrain[[val for val in dfTrain.columns.tolist() if not (val in ['index'])]]\n",
        "  dfTrain.fillna(0.0, inplace=True)\n",
        "  dfDVI = dfTrain.copy()\n",
        "  return dfDVI"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UILMUIp0Kg-9"
      },
      "source": [
        "%cd\n",
        "fileName = os.path.join(dataPath,'model/VirtualAppraiser.zip')\n",
        "ds = ZipFile(fileName)\n",
        "ds.extractall()\n",
        "print('Extracted zip file ' + fileName)\n",
        "\n",
        "%cd ~/VirtualAppraiser\n",
        "fileName = os.path.join(dataPath,'data/visualwebscrapping.zip')\n",
        "os.makedirs('dataset')\n",
        "os.chdir('dataset')\n",
        "ds = ZipFile(fileName)\n",
        "ds.extractall()\n",
        "print('Extracted zip file ' + fileName)\n",
        "\n",
        "%cd ~/VirtualAppraiser\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abj9YMSeV1yq"
      },
      "source": [
        "##### Training Mask R-CNN (Deep Learning) for detecting offer cards in www.fincaraiz.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC1RXz6lC8BU"
      },
      "source": [
        "#Activate only for training\n",
        "'''\n",
        "%cd ~/VirtualAppraiser\n",
        "!python oferta.py train --dataset=dataset/ --weights=coco\n",
        "\n",
        "#Change path of logs\n",
        "shutil.copy('/logs/oferta20210413T2208/mask_rcnn_oferta_0030.h5',os.path.join(dataPath,'OfertaDetector.h5'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL_f5YWGhsZO"
      },
      "source": [
        "##### Inference Mask R-CNN (Deep Learning) for detecting offer cards in www.fincaraiz.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyuB9b6Wqo7"
      },
      "source": [
        "%cd ~/VirtualAppraiser\n",
        "\n",
        "from mrcnn import utils\n",
        "from mrcnn import visualize\n",
        "from mrcnn.visualize import display_images\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn.model import log\n",
        "import oferta\n",
        "\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.getcwd()\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "OfertaDetector = 'OfertaDetector.h5'\n",
        "custom_WEIGHTS_PATH = os.path.join(dataPath, 'model/'+OfertaDetector)\n",
        "\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "config = oferta.OfertaConfig()\n",
        "custom_DIR = os.path.join(ROOT_DIR, \"dataset\")\n",
        "\n",
        "class InferenceConfig(config.__class__):\n",
        "    # Run detection on one image at a time\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "config.display()\n",
        "\n",
        "# Device to load the neural network on.\n",
        "# Useful if you're training a model on the same \n",
        "# machine, in which case use CPU and leave the\n",
        "# GPU for training.\n",
        "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
        "\n",
        "# Inspect the model in training or inference modes\n",
        "# values: 'inference' or 'training'\n",
        "# TODO: code for 'training' test mode not ready yet\n",
        "TEST_MODE = \"inference\"\n",
        "# Create model in inference mode\n",
        "with tensorflow.device(DEVICE):\n",
        "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
        "                              config=config)\n",
        "\n",
        "# Load weights\n",
        "print(\"Loading weights \", custom_WEIGHTS_PATH)\n",
        "\n",
        "model.load_weights(custom_WEIGHTS_PATH, by_name=True)\n",
        "\n",
        "from importlib import reload # was constantly changin the visualization, so I decided to reload it instead of notebook\n",
        "reload(visualize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TugY2iTVYwlR"
      },
      "source": [
        "def ofertaM2Image(frame, model):\n",
        "  # Get ROIS using Mask RCNN\n",
        "  resultsOfertaDetector = model.detect([frame], verbose=0)\n",
        "  potentialcards = resultsOfertaDetector[0]\n",
        "  rois = potentialcards['rois']\n",
        "  scores = potentialcards['scores']\n",
        "  class_ids = potentialcards['class_ids']\n",
        "  image_np = frame.copy()\n",
        "\n",
        "  texOCR = []\n",
        "  # For every ROI make OCR\n",
        "  for roi in rois:\n",
        "    ymin, xmin, ymax, xmax = round(roi[0]), round(roi[1]), round(roi[2]), round(roi[3])\n",
        "    image_np = cv2.rectangle(image_np, (xmin, ymin), (xmax, ymax), (255,0,0), 4)\n",
        "    img_rgb = frame[ymin:ymax,xmin:xmax,:]\n",
        "    texOCR.append(pytesseract.image_to_string(img_rgb))\n",
        "\n",
        "  \n",
        "  # Detect Area info by textual pattern\n",
        "  AreaList = []\n",
        "  for val in texOCR:\n",
        "    x = re.findall(\" m2\", val) \n",
        "    startPos = val.find(x[0])\n",
        "    lastPos = None\n",
        "    i = startPos-1\n",
        "    while i>0:\n",
        "      if val[i] in [' ','\\n']:\n",
        "        lastPos = i\n",
        "        break\n",
        "      else:\n",
        "        i = i - 1\n",
        "    m2 = val[lastPos+1:startPos]\n",
        "    AreaList.append(m2.replace(',','.'))\n",
        "\n",
        "  # Detect Price info by textual pattern\n",
        "  PrecioList = []\n",
        "  for val in texOCR:\n",
        "    x = re.findall(\"\\$ \", val) \n",
        "    startPos = val.find(x[0])\n",
        "    lastPos = None\n",
        "    i = startPos+2\n",
        "\n",
        "    while i<len(val):\n",
        "      if val[i] in [' ','\\n']:\n",
        "        lastPos = i\n",
        "        break\n",
        "      else:\n",
        "        i = i + 1\n",
        "    precio = val[startPos:lastPos]\n",
        "    precio = precio.replace('$ ','')\n",
        "    precio = precio.replace('.','')\n",
        "    PrecioList.append(precio)\n",
        "\n",
        "  # Build $M2\n",
        "  priceM2 = []\n",
        "  if len(AreaList) == len(PrecioList):\n",
        "    for val1,val2 in zip(AreaList,PrecioList):\n",
        "      try:\n",
        "        area = float(val1)\n",
        "        price = float(val2)\n",
        "        priceM2.append(price/area)\n",
        "      except:\n",
        "        print('Invalid values')\n",
        "  return priceM2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0jN5uEuICTp"
      },
      "source": [
        "def identificarInmSimilaresHist(lat, lon, dfDVI, A, NC,NB,NG,NP,E,TI, PET,PCC,PP,PC):\n",
        "  PreciosM2InmSimilaresHistoricos = []\n",
        "  patternReference = [lat,lon,A,NC,NB,NG,E]\n",
        "  patternsMatch = dfDVI[['latitud','longitud','area','habitaciones','banos','garajes','estrato','PriceM2']]\n",
        "  similarityMeasurement = []\n",
        "  for i in range(len(patternsMatch)):\n",
        "    patternMatch = patternsMatch.loc[i,['latitud','longitud','area','habitaciones','banos','garajes','estrato']].tolist()\n",
        "    similarity = 1 - spatial.distance.cosine(patternReference, patternMatch)\n",
        "    similarityMeasurement.append(similarity)\n",
        "  patternsMatch['Similarity'] = similarityMeasurement\n",
        "  patternsMatch = patternsMatch.sort_values(by=['Similarity'],ascending=False)\n",
        "  PreciosM2InmSimilaresHistoricos = patternsMatch['PriceM2'].values.tolist()[:5]\n",
        "  return PreciosM2InmSimilaresHistoricos"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGDJX9HwO_dw"
      },
      "source": [
        "def identificarInmSimilaresOfertados(lat, lon, DVI, A, NC,NB,NG,NP,E,TI, PET,PCC,PP,PC):\n",
        "  PreciosM2InmSimilaresOferta = []\n",
        "  patternReference = [lat,lon,A,NC,NB,NG,E]\n",
        "  #This would be captured by automatic screenshot of webpage using firefox python web driver\n",
        "  frame = cv2.imread(os.path.join(dataPath,'data/Test.png'))\n",
        "  #We need to improve our AI for getting more features\n",
        "  PreciosM2InmSimilaresOferta = ofertaM2Image(frame, model)\n",
        "  return PreciosM2InmSimilaresOferta"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15bWX6SiPW9C"
      },
      "source": [
        "def obtnerIntervaloConfianzaM2(PreciosM2InmSimilaresHistoricos, PreciosM2InmSimilaresOferta, NivelConfianza=0.95):\n",
        "  IntervaloConfianzaM2 = None\n",
        "  data = PreciosM2InmSimilaresHistoricos + PreciosM2InmSimilaresOferta\n",
        "  confidence = NivelConfianza\n",
        "  iterations = 1000\n",
        "  sample_size = 1.0\n",
        "  statistic = np.mean\n",
        "  lower, upper = bootstrap(data, confidence=confidence,\n",
        "                              iterations=iterations,\n",
        "                              sample_size=sample_size,\n",
        "                              statistic=statistic)\n",
        "  IntervaloConfianzaM2 = [lower,upper]\n",
        "  return IntervaloConfianzaM2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33tnZRSSKgXX"
      },
      "source": [
        "# Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oAtU7cmDAa2"
      },
      "source": [
        "DI = 'Cra. 4 # 22-61'         #dirección del inmueble (DI)\n",
        "VCI = 4e6                     #Valor Compra M2 Inmueble (VCI)\n",
        "A = 70                        #área (A)\n",
        "NC = 3                        #número de cuartos (NC)\n",
        "NB = 2                        #número de baños (NB)\n",
        "NG = 1                        #número de parqueadores (NG)\n",
        "NP = 1                        #número de pisos (NP)\n",
        "E = 4                         #estrato (E) \n",
        "TI = 'casa'                   #tipo de inmueble (TI)\n",
        "PET = None                    #polígono de estaciones Transmilenio (PET)\n",
        "PCC = None                    #polígono de centros comerciales (PCC)\n",
        "PP = None                     #polígono de parques (PP)\n",
        "PC = None                     #polígono de CAIs (PC)\n",
        "DVI = 'data/train_data.csv'   #dataset ventas de inmuebles (DVI) \n",
        "PWOI = 'www.fincaraiz.com'    #Página Web de Oferta de Inmuebles (PWOI)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l35o9XxuPO7x"
      },
      "source": [
        "ValorizacionAnual = 1.04"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EHQdKidL23b"
      },
      "source": [
        "## 1. Geodecoding address"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnldOpVqDARc",
        "outputId": "387fb251-48e2-4517-eddf-8549032a1eea"
      },
      "source": [
        "lat,lon = geodecoder(DI)\n",
        "print(lat,lon)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.5799733 -74.0826426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkfcShxsMORN"
      },
      "source": [
        "##2. Historical $M2 (Demand) of houses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gplZgFyNMtzD"
      },
      "source": [
        "dfDVI = pd.read_csv(os.path.join(dataPath,DVI),sep=',',decimal='.')\n",
        "dfDVI = preprocessData(dfDVI)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKMpqRRPHy-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d293cb17-28fe-490e-8d0d-52b35f3f4477"
      },
      "source": [
        "PreciosM2InmSimilaresHistoricos = identificarInmSimilaresHist(lat, lon, dfDVI, A, NC,NB,NG,NP,E,TI, PET,PCC,PP,PC)\n",
        "print(PreciosM2InmSimilaresHistoricos)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3714285.714285714, 4285714.285714285, 4071428.5714285714, 4928571.428571428, 4204285.714285715]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlYzBH5TOLJc"
      },
      "source": [
        "##3. Current $M2 (Supply) of houses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQqeaU9nHy7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b230bda-7bac-4358-f053-b6e95c766977"
      },
      "source": [
        "PreciosM2InmSimilaresOferta = identificarInmSimilaresOfertados(lat, lon, PWOI, A, NC,NB,NG,NP,E,TI, PET,PCC,PP,PC)\n",
        "print(PreciosM2InmSimilaresOferta)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1833333.3333333333, 3600000.0, 3700000.0, 3525000.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsoNxbYcOgee"
      },
      "source": [
        "##4. Confidence Interval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbBu1RHbZpnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6300ac-15bc-4b50-d27f-8c5e9d54fe9f"
      },
      "source": [
        "IntervaloConfianzaM2 = obtnerIntervaloConfianzaM2(PreciosM2InmSimilaresHistoricos, PreciosM2InmSimilaresOferta, NivelConfianza=0.95)\n",
        "print(IntervaloConfianzaM2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3186061.5079365075, 4241684.523809523]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAUhhVYcOsLt"
      },
      "source": [
        "##5. Decision "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaWgLgchOwtm",
        "outputId": "37dda8b3-1542-418b-c8e1-88b7e7173db9"
      },
      "source": [
        "if VCI >= IntervaloConfianzaM2[0] and VCI <= IntervaloConfianzaM2[-1]:\n",
        "  PM2 = IntervaloConfianzaM2[-1]\n",
        "  print('VCI in confidence interval')\n",
        "  print('Then PM2 = %d $/m2'%(round(PM2)))\n",
        "else:\n",
        "  PM2 = VCI * 1.03 * ValorizacionAnual\n",
        "  print('VCI not in confidence interval')\n",
        "  print('Then PM2 = %d $/m2'%(round(PM2)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VCI in confidence interval\n",
            "Then PM2 = 4241685 $/m2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k-KuMGnb1OF"
      },
      "source": [
        "# The END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBZFabIAZ1V1"
      },
      "source": [
        "### Notebook written by: \n",
        "* Anamaría Torres Orduz\n",
        "* Alejandra Torres Orduz\n",
        "* Carlos Diego Ferrin Bolaños\n",
        "\n",
        "April, 2021\n"
      ]
    }
  ]
}